"""Script to prepare a dataset.

Dataset Preparation consists of the following steps:
    0. download raw data (if necessary)
    1. apply data processor
    2. filter dataset
    3. convert to faster data format (NamedTensorDataset)
"""
import os
import hyped
import torch
import datasets
import transformers
import numpy as np
import logging
# utils
from hyped.scripts.utils.data import NamedTensorDataset
from hyped.scripts.utils.configs import RunConfig, DataConfig

logger = logging.getLogger(__name__)


def prepare_dataset(
    ds:datasets.DatasetDict,
    tokenizer:transformers.PreTrainedTokenizer,
    config:DataConfig,
    max_size:int | None =None,
) -> dict[str, datasets.arrow_dataset.Dataset]:

    # reduce datasets if they are too large
    for s, d in ds.items():
        if (max_size is not None) and (len(d) > max_size):
            logger.info("Sampling %s/%s data points from %s split" % (max_size, len(d), s))
            idx = np.random.choice(len(d), max_size, replace=False)
            ds[s] = d.select(idx)

    # get processor and filter type from config
    processor_t = hyped.get_processor_type_from_config(config.processor)
    filter_t = hyped.get_filter_type_from_config(config.filter)
    # create processor and filter instance
    processor = processor_t(tokenizer, config.processor)
    filter_ = filter_t(tokenizer, config.filter)

    logger.info("Applying data processor and filter")
    # preprocess and filter dataset
    ds = ds.prepare_for_task(processor.template) \
        .map(
            function=processor,
            with_indices=processor.requires_index,
            with_rank=processor.requires_rank,
            batched=False,
            load_from_cache_file=False,
            # keep only the columns generated by the processor
            # i.e. remove all columns prior to the processor
            remove_columns=processor.template.features.keys(),
            desc="Processing"
        ) \
        .filter(
            filter_,
            with_indices=filter_.requires_index,
            batched=False,
            load_from_cache_file=False,
            desc="Filtering"
        )

    # set data format to torch
    ds.set_format(type='torch')
    # convert dataset to named tensor dataset
    logger.info("Converting data format")
    return {s: NamedTensorDataset.from_dataset(d) for s, d in ds.items()}

def main():
    from argparse import ArgumentParser
    # build argument parser
    parser = ArgumentParser(description="Prepare dataset for training")
    parser.add_argument("-c", "--config", type=str, required=True, help="Path to run configuration file in .json format")
    parser.add_argument("-n", "--max-size", type=int, default=None, help="Maximum number of data points per split")
    parser.add_argument("-o", "--out-file", type=str, required=True, help="File to store prepared dataset in")
    # parse arguments
    args = parser.parse_args()

    # check if config exists
    if not os.path.isfile(args.config):
        raise FileNotFoundError(args.config)

    # load config
    logger.info("Loading run config from %s" % args.config)
    config = RunConfig.parse_file(args.config)
    # load dataset splits
    logger.info("Downloading/Loading dataset splits")
    ds = datasets.load_dataset(config.data.dataset, split=config.data.splits)
    ds = datasets.DatasetDict(zip(config.data.splits, ds))
    # prepare dataset
    logger.info("Prepareing dataset splits")
    ds = prepare_dataset(ds, config.model.tokenizer, config.data, max_size=args.max_size)

    # save data splits to file
    logger.info("Saving dataset to %s" % args.out_file)
    # create output directory
    os.makedirs(os.path.dirname(args.out_file), exist_ok=True)
    torch.save(ds, args.out_file)

if __name__ == '__main__':
    main()
